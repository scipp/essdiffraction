{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# DREAM: simulation with Si powder sample\n",
    "\n",
    "This notebook illustrates the reduction of Powder Diffraction data for DREAM.\n",
    "\n",
    "- It uses simulated data from McStas/GEANT4 in the High-Flux configuration\n",
    "- The samples were Si powder and Vanadium (for normalisation)\n",
    "\n",
    "For Vanadium, we use the result of a McStas/GEANT4 simulation using the `Incoherent` component to model the Vanadium sample. \n",
    "\n",
    "In this notebook the input files are NeXus files.\n",
    "They created using one of the NeXus files from the CODA pipeline and stored in Scicat and by adding simulated data from McStas/GEANT4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipp as sc\n",
    "import scippneutron as scn\n",
    "import plopp as pp\n",
    "import matplotlib.pyplot as plt\n",
    "import ess.dream as dream\n",
    "from ess.dream import data as dream_data\n",
    "import ipywidgets as ipw\n",
    "import matplotlib.pyplot as plt\n",
    "from scipp.constants import m_n, h\n",
    "import scippnexus as snx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nonfinite_to_zero(\n",
    "    data: sc.DataGroup | sc.DataArray | sc.Dataset\n",
    ") -> sc.DataGroup | sc.DataArray | sc.Dataset:\n",
    "    \"\"\"\n",
    "    Replace NaN, positive and negative infs by zero\n",
    "    \"\"\"\n",
    "    if data.variances is not None:\n",
    "        replacement = sc.scalar(0.0, variance=0.0, unit=data.unit)\n",
    "    else:\n",
    "        replacement = sc.scalar(0.0, unit=data.unit)\n",
    "    return sc.nan_to_num(\n",
    "        data,\n",
    "        nan=replacement,\n",
    "        posinf=replacement,\n",
    "        neginf=replacement)\n",
    "\n",
    "\n",
    "def clean_all_dets(\n",
    "    data: sc.DataGroup | sc.DataArray | sc.Dataset\n",
    "                  ) -> sc.DataGroup | sc.DataArray | sc.Dataset:\n",
    "    \"\"\"Clean the data, removing NaNs.\"\"\"\n",
    "    dg_out = sc.DataGroup()\n",
    "    for item in data.keys():\n",
    "        out = data[item].copy()\n",
    "        out.data = _nonfinite_to_zero(out.data)\n",
    "        dg_out[item] = out\n",
    "    return dg_out\n",
    "\n",
    "\n",
    "def load_dream_nxs(filename: str) -> sc.DataGroup:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    dg_out = sc.DataGroup()\n",
    "    pattern = \"_detector\"\n",
    "    period = (1.0 / sc.scalar(14., unit='Hz')).to(unit='ns')\n",
    "\n",
    "    # temporarily hard-coding values of sample, source and cave monitor positions\n",
    "    # until bug fix for DREAM NeXus file\n",
    "    source_position = sc.vector(value=np.array([-3.478, 0.0, -76550]), unit='mm')\n",
    "    sample_position = sc.vector(value=np.array([0.0, 0.0, 0.0]), unit='mm')\n",
    "\n",
    "    with snx.File(filename) as f:\n",
    "        inst = f['entry']['instrument']\n",
    "        for key in inst:\n",
    "            if key.endswith(pattern):\n",
    "                name = key.removesuffix(pattern)\n",
    "                dg = inst[key][()]\n",
    "                da = dg[f'{name}_event_data']\n",
    "                #dg_out[name]['event'].variances = dg_out[name]['event'].values\n",
    "\n",
    "                # add position\n",
    "                da.coords['position'] = sc.spatial.as_vectors(\n",
    "                    da.coords['x_pixel_offset'],\n",
    "                    da.coords['y_pixel_offset'], \n",
    "                    da.coords['z_pixel_offset'])\n",
    "    \n",
    "                # add sample and source position\n",
    "                da.coords['source_position'] = source_position\n",
    "                da.coords['sample_position'] = sample_position\n",
    "    \n",
    "                # rename ToF\n",
    "                da.bins.coords['tof'] = da.bins.coords.pop('event_time_offset')\n",
    "\n",
    "                # add variances\n",
    "                da.bins.constituents['data'].variances = da.bins.constituents['data'].values\n",
    "            \n",
    "                \n",
    "                # In the raw data, the tofs extend beyond 71ms.\n",
    "                # This is thus not an event_time_offset.\n",
    "                # In addition, there is only one event_time_zero for all events,\n",
    "                # when there should be at least 2 because some tofs are larger than 71ms\n",
    "                \n",
    "                # Bin the data into bins with a 71ms period\n",
    "                da = da.bin(tof=sc.arange('tof', 3) * period)\n",
    "                # Add a event_time_zero coord for each bin, but not as bin edges, as all events in the same pulse have the same event_time_zero, hence the `[:2]`\n",
    "                da.coords['event_time_zero'] = (sc.scalar(1730450434078980000, unit='ns') + da.coords['tof'])[:2]\n",
    "                # Remove the meaningless tof coord at the top level\n",
    "                del da.coords['tof']\n",
    "                # Remove the original (wrong) event_time_zero event coord inside the bins and rename the dim\n",
    "                del da.bins.coords['event_time_zero']\n",
    "                da = da.rename_dims(tof='event_time_zero')\n",
    "                # Compute a proper event_time_offset as tof % period\n",
    "                da.bins.coords['event_time_offset'] = (da.bins.coords.pop('tof') % period).to(unit='us')\n",
    "                dg_out[name] = da\n",
    "        \n",
    "    return dg_out\n",
    "\n",
    "\n",
    "def load_monitor(file: str) -> sc.DataArray:\n",
    "    \"\"\"\n",
    "    Load histogrammed monitor data,\n",
    "    making sure that the time-of-flight wraps around the pulse period.\n",
    "    \"\"\"\n",
    "    tof, tof_count = np.loadtxt(file, usecols=(0, 1),\n",
    "                                skiprows=42, unpack=True)\n",
    "\n",
    "    raw_data =  sc.DataArray(\n",
    "        data=sc.array(dims=['tof'], \n",
    "                      values=tof_count, \n",
    "                      unit='counts'),\n",
    "        coords={\n",
    "            'tof': sc.array(dims=['tof'], values=tof*1000 , unit='ns')\n",
    "        })\n",
    "    raw_data = raw_data['tof', 0.* sc.Unit('ns'):1e8* sc.Unit('ns')].copy()\n",
    "\n",
    "    # Convert coord to bin edges\n",
    "    tof = raw_data.coords['tof']\n",
    "    center = sc.midpoints(tof, dim='tof')\n",
    "    left = center['tof', 0:1] - (tof['tof', 1] - tof['tof', 0])\n",
    "    right = center['tof', -1] + (tof['tof', -1] - tof['tof', -2])\n",
    "    raw_data.coords['tof'] = sc.concat([left, center, right], 'tof')\n",
    "    \n",
    "    period = (1. / sc.scalar(14., unit='Hz')).to(unit=raw_data.coords['tof'].unit)    \n",
    "    bins = sc.sort(sc.concat([raw_data.coords['tof'], period], 'tof'), 'tof')\n",
    "    raw_data = raw_data.rebin(tof=bins)\n",
    "    part1 = raw_data['tof', :period].copy()\n",
    "    part2 = raw_data['tof', period:].copy()\n",
    "    part2.coords['tof'] = part2.coords['tof'] % period\n",
    "    bins = sc.linspace('tof', 0, period.value, 513, unit=period.unit)\n",
    "    \n",
    "    out = part2.rebin(tof=bins) + part1.rebin(tof=bins)\n",
    "    out = out.rename(tof='time_of_flight')\n",
    "\n",
    "    out.coords['Ltotal'] = sc.scalar(np.sqrt(3.478**2 + 72330**2), unit='mm')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Beamline properties\n",
    "\n",
    "We set up the chopper parameters,\n",
    "used to determine the frame boundaries and compute an accurate neutron time-of-flight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sciline as sl\n",
    "from scippneutron.chopper import DiskChopper\n",
    "from scippneutron.tof import unwrap\n",
    "from scippneutron.tof import chopper_cascade\n",
    "\n",
    "psc1 = DiskChopper(\n",
    "    frequency=sc.scalar(14.0, unit=\"Hz\"),\n",
    "    beam_position=sc.scalar(0.0, unit=\"deg\"),\n",
    "    phase=sc.scalar(286 - 180, unit=\"deg\"),\n",
    "    axle_position=sc.vector(value=[0, 0, 6.145], unit=\"m\"),\n",
    "    slit_begin=sc.array(\n",
    "        dims=[\"cutout\"],\n",
    "        values=[-1.23, 70.49, 84.765, 113.565, 170.29, 271.635, 286.035, 301.17],\n",
    "        unit=\"deg\",\n",
    "    ),\n",
    "    slit_end=sc.array(\n",
    "        dims=[\"cutout\"],\n",
    "        values=[1.23, 73.51, 88.035, 116.835, 175.31, 275.565, 289.965, 303.63],\n",
    "        unit=\"deg\",\n",
    "    ),\n",
    "    slit_height=sc.scalar(10.0, unit=\"cm\"),\n",
    "    radius=sc.scalar(30.0, unit=\"cm\"),\n",
    ")\n",
    "\n",
    "psc2 = DiskChopper(\n",
    "    frequency=sc.scalar(-14.0, unit=\"Hz\"),\n",
    "    beam_position=sc.scalar(0.0, unit=\"deg\"),\n",
    "    phase=sc.scalar(-236, unit=\"deg\"),\n",
    "    axle_position=sc.vector(value=[0, 0, 6.155], unit=\"m\"),\n",
    "    slit_begin=sc.array(\n",
    "        dims=[\"cutout\"],\n",
    "        values=[-1.23, 27.0, 55.8, 142.385, 156.765, 214.115, 257.23, 315.49],\n",
    "        unit=\"deg\",\n",
    "    ),\n",
    "    slit_end=sc.array(\n",
    "        dims=[\"cutout\"],\n",
    "        values=[1.23, 30.6, 59.4, 145.615, 160.035, 217.885, 261.17, 318.11],\n",
    "        unit=\"deg\",\n",
    "    ),\n",
    "    slit_height=sc.scalar(10.0, unit=\"cm\"),\n",
    "    radius=sc.scalar(30.0, unit=\"cm\"),\n",
    ")\n",
    "\n",
    "oc = DiskChopper(\n",
    "    frequency=sc.scalar(14.0, unit=\"Hz\"),\n",
    "    beam_position=sc.scalar(0.0, unit=\"deg\"),\n",
    "    phase=sc.scalar(297 - 180 - 90, unit=\"deg\"),\n",
    "    axle_position=sc.vector(value=[0, 0, 6.174], unit=\"m\"),\n",
    "    slit_begin=sc.array(dims=[\"cutout\"], values=[-27.6 * 0.5], unit=\"deg\"),\n",
    "    slit_end=sc.array(dims=[\"cutout\"], values=[27.6 * 0.5], unit=\"deg\"),\n",
    "    slit_height=sc.scalar(10.0, unit=\"cm\"),\n",
    "    radius=sc.scalar(30.0, unit=\"cm\"),\n",
    ")\n",
    "\n",
    "bcc = DiskChopper(\n",
    "    frequency=sc.scalar(112.0, unit=\"Hz\"),\n",
    "    beam_position=sc.scalar(0.0, unit=\"deg\"),\n",
    "    phase=sc.scalar(240 - 180, unit=\"deg\"),\n",
    "    axle_position=sc.vector(value=[0, 0, 9.78], unit=\"m\"),\n",
    "    slit_begin=sc.array(dims=[\"cutout\"], values=[-36.875, 143.125], unit=\"deg\"),\n",
    "    slit_end=sc.array(dims=[\"cutout\"], values=[36.875, 216.875], unit=\"deg\"),\n",
    "    slit_height=sc.scalar(10.0, unit=\"cm\"),\n",
    "    radius=sc.scalar(30.0, unit=\"cm\"),\n",
    ")\n",
    "\n",
    "t0 = DiskChopper(\n",
    "    frequency=sc.scalar(28.0, unit=\"Hz\"),\n",
    "    beam_position=sc.scalar(0.0, unit=\"deg\"),\n",
    "    phase=sc.scalar(280 - 180, unit=\"deg\"),\n",
    "    axle_position=sc.vector(value=[0, 0, 13.05], unit=\"m\"),\n",
    "    slit_begin=sc.array(dims=[\"cutout\"], values=[-314.9 * 0.5], unit=\"deg\"),\n",
    "    slit_end=sc.array(dims=[\"cutout\"], values=[314.9 * 0.5], unit=\"deg\"),\n",
    "    slit_height=sc.scalar(10.0, unit=\"cm\"),\n",
    "    radius=sc.scalar(30.0, unit=\"cm\"),\n",
    ")\n",
    "\n",
    "disk_choppers = {\"psc1\": psc1, \"psc2\": psc2, \"oc\": oc, \"bcc\": bcc, \"t0\": t0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "choppers = {\n",
    "    key: chopper_cascade.Chopper.from_disk_chopper(\n",
    "        chop,\n",
    "        pulse_frequency=sc.scalar(14.0, unit=\"Hz\"),\n",
    "        npulses=1,\n",
    "    )\n",
    "    for key, chop in disk_choppers.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pulse_tmin = sc.scalar(0., unit='ms')\n",
    "pulse_tmax = sc.scalar(5., unit='ms')\n",
    "pulse_wmin = sc.scalar(0.2, unit='angstrom')\n",
    "pulse_wmax = sc.scalar(20., unit='angstrom')\n",
    "\n",
    "frames = chopper_cascade.FrameSequence.from_source_pulse(\n",
    "    time_min=pulse_tmin,\n",
    "    time_max=pulse_tmax,\n",
    "    wavelength_min=pulse_wmin,\n",
    "    wavelength_max=pulse_wmax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chop the frames\n",
    "chopped = frames.chop(choppers.values())\n",
    "\n",
    "# Propagate the neutrons to the detector\n",
    "at_detector = chopped.propagate_to(sc.scalar(76.5, unit='m'))\n",
    "\n",
    "# Visualize the results\n",
    "cascade_fig, cascade_ax = at_detector.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Load and process monitor data\n",
    "\n",
    "For normalisation, we consider the Cave monitor contained in McStas model of DREAM.\n",
    "We load the monitors from the Si and Vanadium simulations.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "*NOTE:*\n",
    "\n",
    "We use the ASCII file from McStas. This is a temporary solution, which will be replaced once monitor data have been added to the NeXus file.\n",
    "\n",
    "</div>\n",
    "\n",
    "### Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tof_mon_cave_si = load_monitor(dream_data.stap_demo_2024_si_monitor())\n",
    "tof_mon_cave_van = load_monitor(dream_data.stap_demo_2024_vanadium_monitor())\n",
    "\n",
    "tof_mon_cave_si.plot(grid=True, title='Si Cave tof monitor') + tof_mon_cave_van.plot(grid=True, title='Vana Cave tof monitor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Convert monitor data to wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup workflow\n",
    "pl = sl.Pipeline((*unwrap.providers(), unwrap.re_histogram_tof_data), params=unwrap.params())\n",
    "pl[unwrap.PulsePeriod] = 1.0 / sc.scalar(14.0, unit=\"Hz\")\n",
    "\n",
    "pl[unwrap.SourceTimeRange] = pulse_tmin, pulse_tmax\n",
    "pl[unwrap.SourceWavelengthRange] = pulse_wmin, pulse_wmax\n",
    "pl[unwrap.Choppers] = choppers\n",
    "\n",
    "# Si sample\n",
    "pl[unwrap.RawData] = tof_mon_cave_si\n",
    "pl[unwrap.Ltotal] = tof_mon_cave_si.coords['Ltotal']\n",
    "tofs_si = pl.compute(unwrap.ReHistogrammedTofData)\n",
    "\n",
    "# Vanadium sample\n",
    "pl[unwrap.RawData] = tof_mon_cave_van\n",
    "pl[unwrap.Ltotal] = tof_mon_cave_van.coords['Ltotal']\n",
    "tofs_van = pl.compute(unwrap.ReHistogrammedTofData)\n",
    "\n",
    "\n",
    "# Convert to wavelength\n",
    "wlgth_graph_monitor = {\n",
    "    **scn.conversion.graph.beamline.beamline(scatter=False),\n",
    "    **scn.conversion.graph.tof.elastic_wavelength('tof'),\n",
    "}\n",
    "\n",
    "mon_cave_si_wlgth = tofs_si.transform_coords('wavelength', graph=wlgth_graph_monitor)\n",
    "mon_cave_van_wlgth = tofs_van.transform_coords('wavelength', graph=wlgth_graph_monitor)\n",
    "\n",
    "# Plot\n",
    "mon_cave_si_wlgth.plot(title='Monitor for Si sample in wavelength', grid=True) + mon_cave_van_wlgth.plot(title='Monitor for Vanadium sample in wavelength', grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Load and process detector data\n",
    "\n",
    "### Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ess.dream import data as dream_data\n",
    "\n",
    "sample = load_dream_nxs(dream_data.stap_demo_2024_si_sample())\n",
    "vana = load_dream_nxs(dream_data.stap_demo_2024_vanadium_sample())\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Conversion to wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_graph = {\n",
    "    **scn.conversion.graph.beamline.beamline(scatter=True),\n",
    "    **scn.conversion.graph.tof.elastic_wavelength('tof'),\n",
    "}\n",
    "\n",
    "dg_si_wlgth = sc.DataGroup()\n",
    "dg_van_wlgth = sc.DataGroup()\n",
    "\n",
    "for (run, out) in ((sample, dg_si_wlgth), (vana, dg_van_wlgth)):\n",
    "    for key, val in run.items():\n",
    "        da = val.transform_coords(['Ltotal'], graph=detector_graph)\n",
    "        pl[unwrap.RawData] = da\n",
    "        pl[unwrap.Ltotal] = da.coords['Ltotal']\n",
    "        \n",
    "        result = pl.compute(unwrap.TofData)\n",
    "        out[key] = result.transform_coords('wavelength', graph=detector_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Normalize by integrated monitor counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: we multiplied the sums by the bin width in the original notebook\n",
    "\n",
    "dg_si_wlgth_norm = dg_si_wlgth / mon_cave_si_wlgth.sum().data\n",
    "dg_van_wlgth_norm = dg_van_wlgth / mon_cave_van_wlgth.sum().data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Convert to d-spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspacing_graph = {\n",
    "      **scn.conversion.graph.beamline.beamline(scatter=True),\n",
    "      **scn.conversion.graph.tof.elastic_dspacing('tof')\n",
    "    }\n",
    "\n",
    "dg_si_dsp = dg_si_wlgth_norm.transform_coords(['dspacing', 'two_theta'], graph=dspacing_graph)\n",
    "dg_van_dsp = dg_van_wlgth_norm.transform_coords(['dspacing', 'two_theta'], graph=dspacing_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Binning in d-spacing and two-theta\n",
    "\n",
    "We consider only the high-resolution and endcap backward detector banks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttheta_bin_edges_per_bank = {}\n",
    "si_norm_bin = sc.DataGroup()\n",
    "van_norm_bin = sc.DataGroup()\n",
    "\n",
    "dspacing_bins = sc.linspace('dspacing', 0.55, 1.75, 601, unit='angstrom')\n",
    "\n",
    "for bank in ['endcap_backward', 'high_resolution']:\n",
    "    \n",
    "    ttheta_bin_edges_per_bank[bank] = (dg_si_dsp[bank].coords['two_theta'].min().value, \n",
    "                                       dg_si_dsp[bank].coords['two_theta'].max().value)\n",
    "\n",
    "    twotheta_bins = sc.linspace('two_theta', \n",
    "                                ttheta_bin_edges_per_bank[bank][0],\n",
    "                                ttheta_bin_edges_per_bank[bank][1],\n",
    "                                181, unit='rad')\n",
    "\n",
    "    si_norm_bin[bank] = dg_si_dsp[bank].bin(two_theta=twotheta_bins, dspacing=dspacing_bins)\n",
    "    van_norm_bin[bank] = dg_van_dsp[bank].bin(two_theta=twotheta_bins, dspacing=dspacing_bins)\n",
    "\n",
    "si_norm_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hr = pp.plot(si_norm_bin['high_resolution'].bins.concat('event_time_zero').hist(), norm='log', title='high_resolution')\n",
    "p_ebwd = pp.plot(si_norm_bin['endcap_backward'].bins.concat('event_time_zero').hist(), norm='log', title='endcap_backward')\n",
    "\n",
    "p_hr / p_ebwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Normalize Si sample data by Vanadium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "si_final_norm = si_norm_bin.hist() / van_norm_bin.hist()\n",
    "cleaned_final_si = clean_all_dets(si_final_norm).sum('event_time_zero')\n",
    "cleaned_final_si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = cleaned_final_si.sum('two_theta').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "theory = np.array([\n",
    "    3.1353, 1.6374, 1.5677, 1.3576, 1.2458, 1.0451, 0.9600, 0.9179, 0.8281, 0.8187,\n",
    "    0.7838, 0.7604, 0.7257, 0.7070, 0.6788, 0.6634, 0.6271, 0.6229, 0.6072, 0.5961,\n",
    "    0.5693, 0.5543, 0.5458, 0.5250, 0.5226])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in theory:\n",
    "    fig.ax.axvline(l, color='gray')\n",
    "fig.canvas.draw()\n",
    "fig.fig.set_size_inches((12., 4.))\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
